{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24d05b3a-aab7-454f-a9f0-052a11c70111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train facts: 1000\n",
      "[Epoch 1] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 2] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 3] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 4] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 5] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 6] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 7] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 8] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 9] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 10] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 11] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 12] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 13] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 14] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 15] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 16] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 17] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 18] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 19] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 20] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 21] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 22] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 23] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 24] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 25] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 26] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 27] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 28] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 29] TrainLoss=0.0000, ValAUC=0.4653\n",
      "[Epoch 30] TrainLoss=0.0000, ValAUC=0.4653\n",
      "66298 !!! len domain_of\n",
      "14962 !!! len type_of\n",
      "1 !!! depth\n",
      "Number of test facts: 500\n",
      "Results written to results3.ttl\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# IMPORTS\n",
    "##############################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# For SPARQL queries, if you want to query the endpoint\n",
    "# pip install SPARQLWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# A) EMBEDDING MODEL (ConvE) + TRAINING CODE\n",
    "##############################################################\n",
    "\n",
    "def parse_reified_triples(ttl_path):\n",
    "    \"\"\"\n",
    "    Parse (s, p, o, label) from the training file which includes hasTruthValue (0 or 1).\n",
    "    \"\"\"\n",
    "    fact_dict = {}\n",
    "    with open(ttl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            # match lines like:\n",
    "            # <factIRI> <predicate> <object> .\n",
    "            # or\n",
    "            # <factIRI> <http://swc2017.aksw.org/hasTruthValue> \"0.0\"^^...\n",
    "            m = re.match(r'^<([^>]+)>\\s+<([^>]+)>\\s+(.*)\\s+\\.$', line)\n",
    "            if not m:\n",
    "                continue\n",
    "            factIRI = m.group(1)\n",
    "            predIRI = m.group(2)\n",
    "            objStr  = m.group(3)\n",
    "\n",
    "            if factIRI not in fact_dict:\n",
    "                fact_dict[factIRI] = {\n",
    "                    'subject': None,\n",
    "                    'predicate': None,\n",
    "                    'object': None,\n",
    "                    'label': None\n",
    "                }\n",
    "\n",
    "            if predIRI == 'http://swc2017.aksw.org/hasTruthValue':\n",
    "                val_match = re.match(r'^\"([^\"]+)\"\\^\\^<([^>]+)>$', objStr)\n",
    "                if val_match:\n",
    "                    val = float(val_match.group(1))\n",
    "                    fact_dict[factIRI]['label'] = val\n",
    "            elif predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#subject':\n",
    "                s_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if s_match:\n",
    "                    fact_dict[factIRI]['subject'] = s_match.group(1)\n",
    "            elif predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate':\n",
    "                p_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if p_match:\n",
    "                    fact_dict[factIRI]['predicate'] = p_match.group(1)\n",
    "            elif predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#object':\n",
    "                o_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if o_match:\n",
    "                    fact_dict[factIRI]['object'] = o_match.group(1)\n",
    "    \n",
    "    results = []\n",
    "    for fid, vals in fact_dict.items():\n",
    "        s = vals['subject']\n",
    "        p = vals['predicate']\n",
    "        o = vals['object']\n",
    "        lbl= vals['label']\n",
    "        if s and p and o and (lbl is not None):\n",
    "            results.append((s, p, o, lbl))\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_index(triples_4tuple):\n",
    "    \"\"\"\n",
    "    Return entity2id, relation2id, plus reverse maps.\n",
    "    \"\"\"\n",
    "    entities = set()\n",
    "    relations = set()\n",
    "    for s, p, o, lbl in triples_4tuple:\n",
    "        entities.add(s)\n",
    "        entities.add(o)\n",
    "        relations.add(p)\n",
    "    entities = sorted(list(entities))\n",
    "    relations = sorted(list(relations))\n",
    "    entity2id = {e: i for i, e in enumerate(entities)}\n",
    "    relation2id = {r: i for i, r in enumerate(relations)}\n",
    "    return entity2id, relation2id\n",
    "\n",
    "\n",
    "def convert_to_idx(triples_4tuple, entity2id, relation2id):\n",
    "    data_idx = []\n",
    "    for s, p, o, lbl in triples_4tuple:\n",
    "        s_idx = entity2id[s]\n",
    "        p_idx = relation2id[p]\n",
    "        o_idx = entity2id[o]\n",
    "        data_idx.append((s_idx, p_idx, o_idx, lbl))\n",
    "    return data_idx\n",
    "\n",
    "\n",
    "class FactDataset(Dataset):\n",
    "    def __init__(self, data_idx, num_entities):\n",
    "        self.data = data_idx\n",
    "        self.num_entities = num_entities\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # (s_idx, p_idx, o_idx, label)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        batch is list of (s, p, o, label)\n",
    "        We'll add 1 negative triple per positive or negative (whichever).\n",
    "        \"\"\"\n",
    "        s = torch.LongTensor([x[0] for x in batch])\n",
    "        p = torch.LongTensor([x[1] for x in batch])\n",
    "        o = torch.LongTensor([x[2] for x in batch])\n",
    "        lbl = torch.FloatTensor([x[3] for x in batch])  # 0 or 1\n",
    "\n",
    "        # We'll generate a random corruption:\n",
    "        batch_size = len(batch)\n",
    "        rand_ents = torch.randint(0, self.num_entities, (batch_size,))\n",
    "        mask_corrupt_head = (torch.rand(batch_size) > 0.5)\n",
    "        s_neg = s.clone()\n",
    "        o_neg = o.clone()\n",
    "        s_neg[mask_corrupt_head] = rand_ents[mask_corrupt_head]\n",
    "        o_neg[~mask_corrupt_head] = rand_ents[~mask_corrupt_head]\n",
    "\n",
    "        # label for corrupted = 0\n",
    "        lbl_neg = torch.zeros(batch_size)\n",
    "\n",
    "        s_final = torch.cat([s, s_neg], dim=0)\n",
    "        p_final = torch.cat([p, p], dim=0)\n",
    "        o_final = torch.cat([o, o_neg], dim=0)\n",
    "        lbl_final= torch.cat([lbl, lbl_neg], dim=0)\n",
    "\n",
    "        return s_final.to(device), p_final.to(device), o_final.to(device), lbl_final.to(device)\n",
    "\n",
    "\n",
    "class ConvE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=100, embed_shape=(10,10), num_filters=32, kernel_size=3):\n",
    "        super(ConvE, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations= num_relations\n",
    "        self.embedding_dim= embedding_dim\n",
    "        assert embed_shape[0]*embed_shape[1] == embedding_dim\n",
    "        self.embed_shape = embed_shape\n",
    "\n",
    "        # Embeddings\n",
    "        self.emb_ent = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.emb_rel = nn.Embedding(num_relations, embedding_dim)\n",
    "        bound = 1.0 / embedding_dim**0.5\n",
    "        nn.init.uniform_(self.emb_ent.weight, a=-bound, b=bound)\n",
    "        nn.init.uniform_(self.emb_rel.weight, a=-bound, b=bound)\n",
    "\n",
    "        # 2D conv\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        out_h = embed_shape[0]*2  # since we stack subject & relation vertically\n",
    "        out_w = embed_shape[1]\n",
    "        self.flat_size = num_filters*out_h*out_w\n",
    "        self.fc = nn.Linear(self.flat_size, embedding_dim)\n",
    "\n",
    "    def forward(self, s_idx, p_idx, o_idx):\n",
    "        \"\"\"\n",
    "        Return logits, shape (B,)\n",
    "        \"\"\"\n",
    "        # embeddings\n",
    "        s_e = self.emb_ent(s_idx)  # (B, dim)\n",
    "        p_e = self.emb_rel(p_idx)  # (B, dim)\n",
    "        o_e = self.emb_ent(o_idx)  # (B, dim)\n",
    "\n",
    "        B = s_e.size(0)\n",
    "        h, w = self.embed_shape\n",
    "\n",
    "        s_2d = s_e.view(B, 1, h, w)\n",
    "        p_2d = p_e.view(B, 1, h, w)\n",
    "        # stack along height => shape (B, 1, 2*h, w)\n",
    "        stacked = torch.cat([s_2d, p_2d], dim=2)\n",
    "\n",
    "        c = self.conv(stacked)   # (B, num_filters, 2h, w)\n",
    "        c = F.relu(c)\n",
    "        c = c.view(B, -1)\n",
    "        c = self.fc(c)\n",
    "        c = F.relu(c)\n",
    "\n",
    "        # dot with o_e\n",
    "        logits = torch.sum(c * o_e, dim=1)\n",
    "        return logits\n",
    "    \n",
    "    def score_triple(self, s_idx, p_idx, o_idx):\n",
    "        \"\"\"\n",
    "        Return a logit for (s_idx, p_idx, o_idx).\n",
    "        Larger => more likely positive.\n",
    "        We'll apply sigmoid outside if needed.\n",
    "        \"\"\"\n",
    "        return self.forward(s_idx, p_idx, o_idx)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# B) LOADING REFERENCE KG & CLASS HIERARCHY\n",
    "##############################################################\n",
    "\n",
    "def load_reference_kg(ref_kg_path):\n",
    "    \"\"\"\n",
    "    Example: parse domain/range statements and any other facts you want from the reference KG dump.\n",
    "    Returns dictionaries:\n",
    "      domain_of: relationIRI -> set of possible domain classes\n",
    "      range_of:  relationIRI -> set of possible range classes\n",
    "      type_of:   entityIRI -> set of classes (rdfs:subClassOf or rdf:type)\n",
    "    You can expand as needed.\n",
    "    \"\"\"\n",
    "    domain_of = {}\n",
    "    range_of  = {}\n",
    "    type_of   = {}\n",
    "    \n",
    "    with open(ref_kg_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            # e.g. <relIRI> <rdfs:domain> <classIRI> .\n",
    "            m = re.match(r'^<([^>]+)>\\s+<([^>]+)>\\s+<([^>]+)>\\s+\\.$', line)\n",
    "            if not m:\n",
    "                continue\n",
    "            subj = m.group(1)   # e.g. relation IRI\n",
    "            pred = m.group(2)\n",
    "            obj  = m.group(3)\n",
    "\n",
    "            if pred == 'http://www.w3.org/2000/01/rdf-schema#domain':\n",
    "                # domain of \"subj\" is \"obj\"\n",
    "                domain_of.setdefault(subj, set()).add(obj)\n",
    "            elif pred == 'http://www.w3.org/2000/01/rdf-schema#range':\n",
    "                # range of \"subj\" is \"obj\"\n",
    "                range_of.setdefault(subj, set()).add(obj)\n",
    "            elif pred == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type':\n",
    "                # type_of[entity].add(class)\n",
    "                type_of.setdefault(subj, set()).add(obj)\n",
    "        print(len(domain_of),\"!!! len domain_of\")\n",
    "        print(len(type_of), \"!!! len type_of\")\n",
    "    return domain_of, range_of, type_of\n",
    "\n",
    "def load_class_hierarchy(class_hierarchy_path):\n",
    "    \"\"\"\n",
    "    Parse lines like:\n",
    "      <classA> <rdfs:subClassOf> <classB> .\n",
    "    We store subClassOf relationships in a dict for inference (transitive closure if desired).\n",
    "    \"\"\"\n",
    "    subclass_of = {}  # e.g. subclass_of[A] = set([B, ...])\n",
    "    with open(class_hierarchy_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            m = re.match(r'^<([^>]+)>\\s+<http://www.w3.org/2000/01/rdf-schema#subClassOf>\\s+<([^>]+)>\\s+\\.$', line)\n",
    "            if not m:\n",
    "                continue\n",
    "            child = m.group(1)\n",
    "            parent= m.group(2)\n",
    "            subclass_of.setdefault(child, set()).add(parent)\n",
    "    return subclass_of\n",
    "\n",
    "def expand_types_with_subclass(type_of, subclass_of, max_depth=5):\n",
    "    \"\"\"\n",
    "    Expand each entity's type set by traversing up the subclass hierarchy.\n",
    "    e.g. if entity e is typed as ChildClass, and ChildClass subClassOf ParentClass,\n",
    "         we also add ParentClass to e's type set.\n",
    "    Simple BFS up to a certain depth or until no more changes.\n",
    "    \"\"\"\n",
    "    for ent, classes in list(type_of.items()):\n",
    "        to_visit = list(classes)\n",
    "        visited = set(classes)\n",
    "        depth=0\n",
    "        while to_visit and depth<max_depth:\n",
    "            nxt = []\n",
    "            for c in to_visit:\n",
    "                # c might have parents\n",
    "                if c in subclass_of:\n",
    "                    for parent in subclass_of[c]:\n",
    "                        if parent not in visited:\n",
    "                            visited.add(parent)\n",
    "                            nxt.append(parent)\n",
    "            to_visit = nxt\n",
    "            depth+=1\n",
    "        # update\n",
    "        type_of[ent] = visited\n",
    "    print(depth,\"!!! depth\")\n",
    "    return type_of\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# C) DOMAIN-RANGE & SUBCLASS REASONING\n",
    "##############################################################\n",
    "\n",
    "def domain_range_consistency(subjectIRI, predicateIRI, objectIRI, \n",
    "                             domain_of, range_of, type_of):\n",
    "    \"\"\"\n",
    "    Return a score in [0,1] indicating how consistent the triple is with\n",
    "    domain/range constraints in the reference KG + type_of dictionary.\n",
    "    Example approach:\n",
    "      - If predicateIRI is in domain_of => check if subjectIRI's types intersect\n",
    "        domain_of[predicateIRI] (or their superclasses).\n",
    "      - If predicateIRI is in range_of => check if objectIRI's types intersect\n",
    "        range_of[predicateIRI].\n",
    "      - Score could be average of domain_match + range_match, or you can do your own logic.\n",
    "    \"\"\"\n",
    "    # if either entity is missing from type_of => we can't confirm or deny => partial\n",
    "    subj_types = type_of.get(subjectIRI, set())\n",
    "    obj_types  = type_of.get(objectIRI, set())\n",
    "\n",
    "    # domain match\n",
    "    doms = domain_of.get(predicateIRI, set())  # possible domain classes\n",
    "    if not doms:\n",
    "        domain_score = 0.2  # we don't know domain => partial guess\n",
    "    else:\n",
    "        # if there's an intersection between subj_types and doms => good\n",
    "        if subj_types.intersection(doms):\n",
    "            domain_score = 1.0\n",
    "        else:\n",
    "            domain_score = 0.0\n",
    "\n",
    "    # range match\n",
    "    rngs = range_of.get(predicateIRI, set())\n",
    "    if not rngs:\n",
    "        range_score = 0.2\n",
    "    else:\n",
    "        if obj_types.intersection(rngs):\n",
    "            range_score = 1.0\n",
    "        else:\n",
    "            range_score = 0.0\n",
    "\n",
    "    # final\n",
    "    # we can do the average or min or something\n",
    "    final_score = (domain_score + range_score)/2.0\n",
    "    return final_score\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# D) SCORING LOGIC (COMBINING EMBEDDINGS + KNOWLEDGE-BASED CHECKS)\n",
    "##############################################################\n",
    "\n",
    "def hybrid_score(model, \n",
    "                 sIRI, pIRI, oIRI, \n",
    "                 entity2id, relation2id, \n",
    "                 domain_of, range_of, type_of,\n",
    "                 alpha=0.9):\n",
    "    \"\"\"\n",
    "    If (sIRI, pIRI, oIRI) is fully in-vocab => produce embedding score in [0,1].\n",
    "    If partially OOV => combine domain-range check with fallback to .5 or partial embedding.\n",
    "\n",
    "    alpha controls how strongly we weigh the knowledge-based check vs. embedding.\n",
    "    \"\"\"\n",
    "    # 1) Check if in-vocab\n",
    "    inVocab = (sIRI in entity2id) and (pIRI in relation2id) and (oIRI in entity2id)\n",
    "\n",
    "    # 2) Knowledge-based domain-range check\n",
    "    kg_consistency = domain_range_consistency(sIRI, pIRI, oIRI, domain_of, range_of, type_of)\n",
    "\n",
    "    if inVocab:\n",
    "        # Embedding-based logit => convert to [0,1] by sigmoid\n",
    "        s_idx = torch.tensor([entity2id[sIRI]], device=device)\n",
    "        p_idx = torch.tensor([relation2id[pIRI]], device=device)\n",
    "        o_idx = torch.tensor([entity2id[oIRI]], device=device)\n",
    "        with torch.no_grad():\n",
    "            logit = model.score_triple(s_idx, p_idx, o_idx)  # shape (1,)\n",
    "            emb_score = torch.sigmoid(logit).item()\n",
    "        # combine with domain-range if you want\n",
    "        final_score = alpha*emb_score + (1-alpha)*kg_consistency\n",
    "    else:\n",
    "        # at least one is OOV => we have no embedding info\n",
    "        # => fallback to knowledge-based check alone or combine with 0.5\n",
    "        final_score = alpha*kg_consistency + (1-alpha)*0.5\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# E) GENERATE FINAL TTL (TEST SET) \n",
    "##############################################################\n",
    "\n",
    "def parse_test_reified_triples(ttl_path):\n",
    "    \"\"\"\n",
    "    Return list of (factIRI, subjectIRI, predicateIRI, objectIRI) from test file\n",
    "    (No hasTruthValue line).\n",
    "    \"\"\"\n",
    "    fact_dict={}\n",
    "    with open(ttl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            m = re.match(r'^<([^>]+)>\\s+<([^>]+)>\\s+(.*)\\s+\\.$', line)\n",
    "            if not m:\n",
    "                continue\n",
    "            factIRI = m.group(1)\n",
    "            predIRI = m.group(2)\n",
    "            objStr  = m.group(3)\n",
    "            if factIRI not in fact_dict:\n",
    "                fact_dict[factIRI]={'subject':None, 'predicate':None, 'object':None}\n",
    "            if predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#subject':\n",
    "                s_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if s_match:\n",
    "                    fact_dict[factIRI]['subject']= s_match.group(1)\n",
    "            elif predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate':\n",
    "                p_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if p_match:\n",
    "                    fact_dict[factIRI]['predicate']= p_match.group(1)\n",
    "            elif predIRI == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#object':\n",
    "                o_match = re.match(r'^<([^>]+)>$', objStr)\n",
    "                if o_match:\n",
    "                    fact_dict[factIRI]['object']= o_match.group(1)\n",
    "            # ignore type statement, etc.\n",
    "\n",
    "    results=[]\n",
    "    for fid,vals in fact_dict.items():\n",
    "        s=vals['subject']\n",
    "        p=vals['predicate']\n",
    "        o=vals['object']\n",
    "        if s and p and o:\n",
    "            results.append((fid, s, p, o))\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_results_ttl(model, \n",
    "                         test_path, \n",
    "                         out_path, \n",
    "                         entity2id, \n",
    "                         relation2id, \n",
    "                         domain_of, \n",
    "                         range_of, \n",
    "                         type_of,\n",
    "                         alpha=0.9):\n",
    "    \"\"\"\n",
    "    For each test fact, produce a final veracity in [0,1], then write to out_path as TTL lines.\n",
    "    \"\"\"\n",
    "    test_facts = parse_test_reified_triples(test_path)\n",
    "    print(\"Number of test facts:\", len(test_facts))\n",
    "\n",
    "    model.eval()\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        for (factIRI, sIRI, pIRI, oIRI) in test_facts:\n",
    "            score = hybrid_score(model, sIRI, pIRI, oIRI, \n",
    "                                 entity2id, relation2id,\n",
    "                                 domain_of, range_of, type_of,\n",
    "                                 alpha=alpha)\n",
    "            # write line\n",
    "            line = f'<{factIRI}> <http://swc2017.aksw.org/hasTruthValue> \"{score}\"^^<http://www.w3.org/2001/XMLSchema#double> .\\n'\n",
    "            f.write(line)\n",
    "    print(f\"Results written to {out_path}\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# MAIN PIPELINE EXAMPLE\n",
    "##############################################################\n",
    "\n",
    "def main():\n",
    "    #######################\n",
    "    # 1) Load + Train Model\n",
    "    #######################\n",
    "    training_ttl = \"fokg-sw-train-2024.nt\"  # adapt\n",
    "    all_facts = parse_reified_triples(training_ttl)\n",
    "    print(\"Train facts:\", len(all_facts))\n",
    "\n",
    "    entity2id, relation2id = build_index(all_facts)\n",
    "    data_idx = convert_to_idx(all_facts, entity2id, relation2id)\n",
    "\n",
    "    # Shuffle & split\n",
    "    random.shuffle(data_idx)\n",
    "    split_pt = int(0.8*len(data_idx))\n",
    "    train_data = data_idx[:split_pt]\n",
    "    valid_data = data_idx[split_pt:]\n",
    "\n",
    "    train_ds = FactDataset(train_data, num_entities=len(entity2id))\n",
    "    valid_ds = FactDataset(valid_data, num_entities=len(entity2id))\n",
    "\n",
    "    train_loader= DataLoader(train_ds, batch_size=1024, shuffle=True, \n",
    "                             collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "    valid_loader= DataLoader(valid_ds, batch_size=1024, shuffle=False, \n",
    "                             collate_fn=valid_ds.collate_fn, drop_last=False)\n",
    "\n",
    "    model = ConvE(num_entities=len(entity2id),\n",
    "                  num_relations=len(relation2id),\n",
    "                  embedding_dim=100,\n",
    "                  embed_shape=(10,10),\n",
    "                  num_filters=32,\n",
    "                  kernel_size=3).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    EPOCHS=30\n",
    "\n",
    "    def train_epoch(loader):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for batch in loader:\n",
    "            s, p, o, lbl = batch\n",
    "            logits = model.forward(s, p, o)\n",
    "            loss   = F.binary_cross_entropy_with_logits(logits, lbl)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss+=loss.item()\n",
    "        return total_loss\n",
    "    \n",
    "    def evaluate_auc(loader):\n",
    "        model.eval()\n",
    "        all_labels=[]\n",
    "        all_scores=[]\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                s, p, o, lbl= batch\n",
    "                batch_size = s.shape[0]//2\n",
    "                # first half => real\n",
    "                s_real= s[:batch_size]\n",
    "                p_real= p[:batch_size]\n",
    "                o_real= o[:batch_size]\n",
    "                lbl_real=lbl[:batch_size]\n",
    "                logits= model.forward(s_real, p_real, o_real)\n",
    "                all_scores.extend(logits.cpu().numpy().tolist())\n",
    "                all_labels.extend(lbl_real.cpu().numpy().tolist())\n",
    "        # check if we have both 0 and 1\n",
    "        if len(set(all_labels))<2:\n",
    "            return 0.0\n",
    "        return roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss= train_epoch(train_loader)\n",
    "        val_auc= evaluate_auc(valid_loader)\n",
    "        print(f\"[Epoch {epoch}] TrainLoss={train_loss:.4f}, ValAUC={val_auc:.4f}\")\n",
    "\n",
    "    #######################\n",
    "    # 2) Load Reference KG + Class Hierarchy\n",
    "    #######################\n",
    "    ref_kg_path= \"reference-kg.nt\"  # adapt\n",
    "    domain_of, range_of, type_of = load_reference_kg(ref_kg_path)\n",
    "\n",
    "    class_hierarchy_path= \"classHierarchy.nt\" # adapt\n",
    "    subclass_of= load_class_hierarchy(class_hierarchy_path)\n",
    "\n",
    "    # Expand type_of with transitive subClassOf\n",
    "    type_of= expand_types_with_subclass(type_of, subclass_of)\n",
    "\n",
    "    #######################\n",
    "    # 3) Predict on Test\n",
    "    #######################\n",
    "    test_ttl= \"fokg-sw-test-2024.nt\"\n",
    "    out_ttl= \"results3.ttl\"\n",
    "\n",
    "    generate_results_ttl(model, \n",
    "                         test_ttl, \n",
    "                         out_ttl, \n",
    "                         entity2id,\n",
    "                         relation2id,\n",
    "                         domain_of,\n",
    "                         range_of,\n",
    "                         type_of,\n",
    "                         alpha=0.9)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739750e-04be-465f-9cba-fc9efb54cb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeca66e-4bf3-47ab-b17c-a6a3d6603d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a518fc4-a0e1-4615-ad25-9ce46627b5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
